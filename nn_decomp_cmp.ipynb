{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage.morphology import dilation\n",
    "from skimage.morphology import disk\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "ds = xr.open_dataset(\"Murrumbidgee_near_Bundure__MUR_B3.nc\")\n",
    "ds = ds.isel(x=slice(400,800), y=slice(0,400))\n",
    "\n",
    "ids = ds.where((ds.nbart_blue - ds.nbart_blue.quantile(0.25, dim='time'))<1000)\n",
    "ids = ids.isel(time=(np.count_nonzero(~np.isnan(ids.nbart_blue.values), axis=(1,2)))>400*400*.66)\n",
    "ids = ids.rolling(time=7, min_periods=3, center=True).median()\n",
    "ids = ids.reindex({\"time\": ds.time})\n",
    "ids = ids.interpolate_na(dim='time', method='nearest', fill_value='extrapolate')\n",
    "\n",
    "mask = (ds.nbart_blue - ids.nbart_blue) > 100 + ids.nbart_blue/2\n",
    "mask += (ds.nbart_nir_2 - ids.nbart_nir_2) < -600 + ids.nbart_nir_2/16\n",
    "mask = mask.values\n",
    "\n",
    "for t in range(mask.shape[0]):\n",
    "    mask[t] = remove_small_objects(mask[t], 9)\n",
    "    mask[t] = dilation(mask[t], disk(9))\n",
    "\n",
    "ti_nan = (np.count_nonzero(mask, axis=(1,2)))<.66*160000\n",
    "ds = ds.where(~mask).isel(time=ti_nan)\n",
    "\n",
    "ds[['nbart_red','nbart_green','nbart_blue']].clip(0,2200).to_array().plot.imshow(col='time', col_wrap=6, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask[ti_nan]\n",
    "\n",
    "def generate_patches(patch_hsize):\n",
    "    i = int(np.random.randint(patch_hsize, 400-patch_hsize, size=1))\n",
    "    j = int(np.random.randint(patch_hsize, 400-patch_hsize, size=1))\n",
    "    t = int(np.random.randint(mask.shape[0], size=1))\n",
    "    \n",
    "    while np.count_nonzero(~mask[t, j-patch_hsize:j+patch_hsize, i-patch_hsize:i+patch_hsize]) != (2*patch_hsize)**2:\n",
    "        t = int(np.random.randint(mask.shape[0], size=1))\n",
    "        i = int(np.random.randint(patch_hsize, 400-patch_hsize, size=1))\n",
    "        j = int(np.random.randint(patch_hsize, 400-patch_hsize, size=1))\n",
    "                    \n",
    "    return t, i, j\n",
    "\n",
    "patch_hsize = 40\n",
    "\n",
    "pmask = np.zeros(mask.shape, dtype=np.bool)\n",
    "\n",
    "for _ in range(50):\n",
    "    t, i, j = generate_patches(patch_hsize)\n",
    "    pmask[t, j-patch_hsize:j+patch_hsize, i-patch_hsize:i+patch_hsize] = True\n",
    "    \n",
    "ds[['nbart_red','nbart_green','nbart_blue']].where(~pmask).clip(0,2200).to_array().plot.imshow(col='time', col_wrap=6, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = ds[\"nbart_nir_1\"].astype(np.float32) / 1e4\n",
    "\n",
    "pstack = stack.where(~pmask)\n",
    "istack = pstack.interpolate_na(dim='time')\n",
    "istack = istack.interpolate_na(dim='time', method='nearest', fill_value='extrapolate')\n",
    "\n",
    "stack = stack.values.reshape(stack.shape[0], -1)\n",
    "pstack = pstack.values.reshape(stack.shape[0], -1)\n",
    "istack = istack.values.reshape(stack.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack[~pmask.reshape(-1,160000)] = np.nan\n",
    "\n",
    "plt.imshow(stack[1].reshape(400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=12).fit(istack)\n",
    "coeffs = pca.transform(istack)\n",
    "pca_decomp = pca.inverse_transform(coeffs)\n",
    "pca_decomp[~pmask.reshape(-1,160000)]=np.nan\n",
    "\n",
    "np.nanmean(np.square(pca_decomp-stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_coeffs):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.fc1 = nn.Linear(1, self.n_coeffs, bias=False)\n",
    "        self.fc2 = nn.Linear(1, 160000, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        coeffs = self.fc1(x)\n",
    "        base = self.fc2(x)\n",
    "        return torch.matmul(coeffs.unsqueeze(1), base.unsqueeze(0))\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ncoeffs = stack.shape[0]\n",
    "\n",
    "input = torch.ones(1, device=device)\n",
    "tmean = np.nanmean(stack, axis=0)\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "pcs = np.zeros((160000,12))\n",
    "cfs = np.zeros((ncoeffs,12))\n",
    "\n",
    "for pc_i in range(12):\n",
    "    net = Net(ncoeffs)\n",
    "    net.to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "    prev_loss = 10.0\n",
    "    patience = 0\n",
    "    for it in range(10000):\n",
    "        # training loop:\n",
    "        output = net(input)\n",
    "        loss = nan_mse_loss(output, target)\n",
    "\n",
    "        # patience \n",
    "        if (prev_loss-loss.item()) < 1e-10:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "\n",
    "        if patience == 100:\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss.backward()\n",
    "        optimizer.step()    # does the update\n",
    "\n",
    "        prev_loss = loss.item()\n",
    "    \n",
    "    print(pc_i, it)\n",
    "\n",
    "    params = list(net.parameters())\n",
    "    cfs[:,pc_i:pc_i+1] = params[0].cpu().detach().numpy()\n",
    "    pcs[:,pc_i:pc_i+1] = params[1].cpu().detach().numpy()\n",
    "\n",
    "    residual = target.cpu().detach().numpy() - cfs[:,pc_i:pc_i+1]@pcs[:,pc_i:pc_i+1].T\n",
    "    target = torch.from_numpy(residual).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rec = cfs@pcs.T\n",
    "\n",
    "\n",
    "plt.imshow(rec[1].reshape(400,400))\n",
    "\n",
    "np.nanmean(np.square((rec+tmean)-stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_coeffs, n_comps, n_pix):\n",
    "        super(MF, self).__init__()\n",
    "        self.cfs = nn.Parameter(torch.rand(n_coeffs, n_comps, requires_grad=True))\n",
    "        self.cmps = nn.Parameter(torch.rand(n_comps, n_pix, requires_grad=True))\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.matmul(self.cfs,self.cmps)\n",
    "\n",
    "    \n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ncomps = 12\n",
    "ncoeffs = stack.shape[0]\n",
    "npix = 160000\n",
    "\n",
    "net = MF(ncoeffs, ncomps, npix)\n",
    "net.to(device)\n",
    "tmean = np.nanmean(pstack, axis=0)\n",
    "target = torch.from_numpy(pstack-tmean).float().to(device)\n",
    "\n",
    "opt = optim.AdamW(net.parameters(), lr=1.0)\n",
    "\n",
    "n_epoch  = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    yhat = net()\n",
    "    loss = nan_mse_loss(yhat, target) #+ λ*unit_norm(net.cmps)# + μ*torch.norm(net.cfs, p=1)\n",
    "\n",
    "    net.zero_grad() # need to clear the old gradients\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.cfs.data = net.cfs.data*torch.norm(net.cmps, dim=1).data/20\n",
    "    net.cmps.data = net.cmps.data/torch.norm(net.cmps, dim=1).data[:,None]*20\n",
    "\n",
    "\n",
    "opt = optim.AdamW(net.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch  = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    yhat = net()\n",
    "    loss = nan_mse_loss(yhat, target) #+ λ*unit_norm(net.cmps)# + μ*torch.norm(net.cfs, p=1)\n",
    "\n",
    "    net.zero_grad() # need to clear the old gradients\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.cfs.data = net.cfs.data*torch.norm(net.cmps, dim=1).data/20\n",
    "    net.cmps.data = net.cmps.data/torch.norm(net.cmps, dim=1).data[:,None]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec2 = net().cpu().detach().numpy()+tmean\n",
    "rec2[~pmask.reshape(-1,160000)]=np.nan\n",
    "\n",
    "np.nanmean(np.square(rec2-stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
