{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 0.753046989440918 0.753046989440918\n",
      "100 0.00030304433312267065 0.00030304433312267065\n",
      "200 0.00018829345935955644 0.00018829345935955644\n",
      "300 0.00013131002197042108 0.00013131002197042108\n",
      "400 9.09754671738483e-05 9.09754671738483e-05\n",
      "500 7.90017656981945e-05 7.90017656981945e-05\n",
      "600 7.052533328533173e-05 7.052533328533173e-05\n",
      "700 6.422973092412576e-05 6.422973092412576e-05\n",
      "800 5.803992462460883e-05 5.803992462460883e-05\n",
      "900 5.349063576431945e-05 5.349063576431945e-05\n",
      "1000 4.939973587170243e-05 4.939973587170243e-05\n",
      "1100 4.571126191876829e-05 4.571126191876829e-05\n",
      "1200 4.285776594770141e-05 4.285776594770141e-05\n",
      "1300 4.0531645936425775e-05 4.0531645936425775e-05\n",
      "1400 3.898710201610811e-05 3.898710201610811e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*404*404, bias=False)\n",
    "        self.conv1 = nn.Conv2d(self.n_comps, 2*self.n_comps, kernel_size=5, bias=False)\n",
    "        self.coeffs = nn.Linear(1, 2*self.n_comps*self.n_coeffs, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        conv1 =  torch.tanh(self.conv1(base.reshape(1, self.n_comps, 404, 404)))\n",
    "        coeffs = self.coeffs(x).view(2*self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ki,kj->ji', conv1.view(2*self.n_comps,400*400), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ds = xr.open_dataset(\"/data/pca_act/000_clean.nc\")\n",
    "ti_nan = (np.count_nonzero(np.isnan(ds.nbart_blue.values), axis=(1,2)))<.66*160000\n",
    "ds = ds.isel(time=ti_nan)\n",
    "\n",
    "stack = np.empty((0,400,400))\n",
    "for fname in ds:\n",
    "    band = ds[fname].values/1e4\n",
    "    stack = np.append(stack, band, axis=0)\n",
    "\n",
    "stack = stack.reshape(stack.shape[0], -1)\n",
    "\n",
    "ncomps = 12\n",
    "ncoeffs = stack.shape[0]\n",
    "\n",
    "input = torch.ones(1, device=device)\n",
    "tmean = np.nanmean(stack, axis=0)\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "net = AE(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 1500\n",
    "# training loop:\n",
    "for it in range(epochs):\n",
    "    output = net(input)\n",
    "\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print(it, loss.item(), nan_mse_loss(output, target).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006083517801016569\n",
      "0.006080227438360453\n",
      "0.005994681268930435\n",
      "0.004141979850828648\n",
      "0.0006812543724663556\n",
      "0.0005685978685505688\n",
      "0.0005585349281318486\n",
      "0.0005523429717868567\n",
      "0.000541805406101048\n",
      "0.0005210671224631369\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.base = torch.nn.init.xavier_uniform_(torch.empty(n_comps, 400*400, requires_grad=True)).to(device)\n",
    "        self.coeffs = torch.nn.init.xavier_uniform_(torch.empty(n_coeffs, n_comps, requires_grad=True)).to(device)\n",
    "    def forward(self):\n",
    "        return torch.mm(self.coeffs, self.base)\n",
    "    \n",
    "net = AE(ncomps, ncoeffs)\n",
    "#net.to(device)\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "for i in range(100):\n",
    "    loss = nan_mse_loss(net(), target)\n",
    "    g_base, = grad(loss, net.base)\n",
    "    with torch.no_grad():\n",
    "        net.base -= 10000*g_base\n",
    "        \n",
    "    loss = nan_mse_loss(net(), target)  \n",
    "    g_coeffs, = grad(loss, net.coeffs)\n",
    "    with torch.no_grad():\n",
    "        net.coeffs -= 10000*g_coeffs\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008309215772897005\n",
      "0.0008298605098389089\n",
      "0.0008291483391076326\n",
      "0.0008285042131319642\n",
      "0.000827873358502984\n",
      "0.0008272448903881013\n",
      "0.0008266165386885405\n",
      "0.0008259877213276923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-8e3cae354934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnan_mse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mg_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         inputs, allow_unused)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    output = net()\n",
    "    loss = nan_mse_loss(output, target)\n",
    "    g_base, = grad(loss, net.base)\n",
    "    with torch.no_grad():\n",
    "        net.base -= 100*g_base\n",
    "        \n",
    "    output = net()\n",
    "    loss = nan_mse_loss(output, target)  \n",
    "    g_coeffs, = grad(loss, net.coeffs)\n",
    "    with torch.no_grad():\n",
    "        net.coeffs -= 100*g_coeffs\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.353246808052063\n",
      "10 0.014761357568204403\n",
      "20 0.024680428206920624\n",
      "30 0.011752721853554249\n",
      "40 0.004274691455066204\n",
      "50 0.001417412655428052\n",
      "60 0.0008546735625714064\n",
      "70 0.0006324619753286242\n",
      "80 0.0005126555915921926\n",
      "90 0.0003928365185856819\n",
      "100 0.0003020345466211438\n",
      "110 0.0002592696400824934\n",
      "120 0.00023512639745604247\n",
      "130 0.00021287819254212081\n",
      "140 0.00019151439482811838\n",
      "150 0.00017375645984429866\n",
      "160 0.00015989817620720714\n",
      "170 0.00014828499115537852\n",
      "180 0.00013794457481708378\n",
      "190 0.00012905357289128006\n",
      "200 0.0001218087927554734\n",
      "210 0.00011587271001189947\n",
      "220 0.00011068936146330088\n",
      "230 0.0001058629568433389\n",
      "240 0.00010125763219548389\n",
      "250 9.692672028904781e-05\n",
      "260 9.299091470893472e-05\n",
      "270 8.954280201578513e-05\n",
      "280 8.660915773361921e-05\n",
      "290 8.415898628300056e-05\n",
      "300 8.212956163333729e-05\n",
      "310 8.044940477702767e-05\n",
      "320 7.905159873189405e-05\n",
      "330 7.78788817115128e-05\n",
      "340 7.688413461437449e-05\n",
      "350 7.602925325045362e-05\n",
      "360 7.528356945840642e-05\n",
      "370 7.46224686736241e-05\n",
      "380 7.402637129416689e-05\n",
      "390 7.347980863414705e-05\n",
      "400 7.297070987988263e-05\n",
      "410 7.248968904605135e-05\n",
      "420 7.20295065548271e-05\n",
      "430 7.158453081501648e-05\n",
      "440 7.115031621651724e-05\n",
      "450 7.072323933243752e-05\n",
      "460 7.030031702015549e-05\n",
      "470 6.987897359067574e-05\n",
      "480 6.945704080862924e-05\n",
      "490 6.90327215124853e-05\n"
     ]
    }
   ],
   "source": [
    "class AE2(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE2, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*400*400, bias=False)\n",
    "        self.coeffs = nn.Linear(1, self.n_comps*self.n_coeffs, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        coeffs = self.coeffs(x).view(self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ki,kj->ji', base.view(self.n_comps,400*400), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "net = AE2(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.1)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "for i in range(1500):\n",
    "    output = net(input)\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
