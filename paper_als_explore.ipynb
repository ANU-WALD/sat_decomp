{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 0.734163224697113 0.734163224697113\n",
      "100 0.0005438955849967897 0.0005438955849967897\n",
      "200 0.00016100543143693358 0.00016100543143693358\n",
      "300 0.00012727727880701423 0.00012727727880701423\n",
      "400 0.00010232954809907824 0.00010232954809907824\n",
      "500 8.325299131684005e-05 8.325299131684005e-05\n",
      "600 7.523925160057843e-05 7.523925160057843e-05\n",
      "700 6.876718543935567e-05 6.876718543935567e-05\n",
      "800 6.202853546710685e-05 6.202853546710685e-05\n",
      "900 5.60833650524728e-05 5.60833650524728e-05\n",
      "1000 5.081657218397595e-05 5.081657218397595e-05\n",
      "1100 4.695257302955724e-05 4.695257302955724e-05\n",
      "1200 4.4398955651558936e-05 4.4398955651558936e-05\n",
      "1300 4.26465121563524e-05 4.26465121563524e-05\n",
      "1400 4.1008766856975853e-05 4.1008766856975853e-05\n",
      "1500 3.925561759388074e-05 3.925561759388074e-05\n",
      "1600 3.7734080251539126e-05 3.7734080251539126e-05\n",
      "1700 7.869963155826554e-05 7.869963155826554e-05\n",
      "1800 6.348744500428438e-05 6.348744500428438e-05\n",
      "1900 4.4255804823478684e-05 4.4255804823478684e-05\n",
      "2000 3.359564652782865e-05 3.359564652782865e-05\n",
      "2100 3.7687110307160765e-05 3.7687110307160765e-05\n",
      "2200 3.720871973200701e-05 3.720871973200701e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-0d200b2e30cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*404*404, bias=False)\n",
    "        self.conv1 = nn.Conv2d(self.n_comps, 2*self.n_comps, kernel_size=5, bias=False)\n",
    "        self.coeffs = nn.Linear(1, 2*self.n_comps*self.n_coeffs, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        conv1 =  torch.tanh(self.conv1(base.reshape(1, self.n_comps, 404, 404)))\n",
    "        coeffs = self.coeffs(x).view(2*self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ki,kj->ji', conv1.view(2*self.n_comps,400*400), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ds = xr.open_dataset(\"/data/pca_act/000_clean.nc\")\n",
    "ti_nan = (np.count_nonzero(np.isnan(ds.nbart_blue.values), axis=(1,2)))<.66*160000\n",
    "ds = ds.isel(time=ti_nan)\n",
    "\n",
    "stack = np.empty((0,400,400))\n",
    "for fname in ds:\n",
    "    band = ds[fname].values/1e4\n",
    "    stack = np.append(stack, band, axis=0)\n",
    "\n",
    "stack = stack.reshape(stack.shape[0], -1)\n",
    "\n",
    "ncomps = 12\n",
    "ncoeffs = stack.shape[0]\n",
    "\n",
    "input = torch.ones(1, device=device)\n",
    "tmean = np.nanmean(stack, axis=0)\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "net = AE(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 3000\n",
    "# training loop:\n",
    "for it in range(epochs):\n",
    "    output = net(input)\n",
    "\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print(it, loss.item(), nan_mse_loss(output, target).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0003, -0.0010, -0.0033,  ...,  0.0015,  0.0048, -0.0022],\n",
       "         [-0.0059,  0.0052,  0.0010,  ...,  0.0032,  0.0010,  0.0013],\n",
       "         [-0.0056, -0.0002, -0.0011,  ...,  0.0025, -0.0038,  0.0007],\n",
       "         ...,\n",
       "         [ 0.0046, -0.0006, -0.0018,  ..., -0.0039,  0.0041,  0.0043],\n",
       "         [ 0.0039, -0.0019,  0.0059,  ..., -0.0042,  0.0024,  0.0021],\n",
       "         [-0.0004,  0.0035,  0.0053,  ..., -0.0056, -0.0027, -0.0057]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0405,  0.0590,  0.0506,  ...,  0.0668, -0.0500,  0.0060],\n",
       "         [-0.0047, -0.0524, -0.0117,  ...,  0.0526,  0.0260,  0.0896],\n",
       "         [-0.0240, -0.0182,  0.0787,  ...,  0.0547,  0.0570, -0.0848],\n",
       "         ...,\n",
       "         [-0.0180, -0.0366, -0.0491,  ...,  0.0141,  0.0815,  0.0371],\n",
       "         [ 0.1068,  0.0304, -0.0873,  ...,  0.1058,  0.0052,  0.0790],\n",
       "         [ 0.0033,  0.0068,  0.0543,  ...,  0.0299, -0.0566, -0.0940]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.base = torch.nn.Parameter(torch.empty(n_comps, 400*400, requires_grad=True))#.to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.base)\n",
    "        self.coeffs = torch.nn.Parameter(torch.empty(n_coeffs, n_comps, requires_grad=True))#.to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.coeffs)\n",
    "    def forward(self):\n",
    "        return torch.mm(self.coeffs, self.base)\n",
    "    \n",
    "net = AE(ncomps, ncoeffs)\n",
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-37ae674d6e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#optimizer = optim.Adam(net.parameters(), lr=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     45\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     46\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.base = torch.nn.Parameter(torch.empty(n_comps, 400*400, requires_grad=True)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.base)\n",
    "        self.coeffs = torch.nn.Parameter(torch.empty(n_coeffs, n_comps, requires_grad=True)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(self.coeffs)\n",
    "    def forward(self):\n",
    "        return torch.mm(self.coeffs, self.base)\n",
    "    \n",
    "net = AE(ncomps, ncoeffs)\n",
    "#net.to(device)\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.1)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for i in range(3000):\n",
    "    output = net(input)\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.item())\n",
    "\n",
    "\"\"\"\n",
    "for i in range(100):\n",
    "    loss = nan_mse_loss(net(), target)\n",
    "    g_base, = grad(loss, net.base)\n",
    "    with torch.no_grad():\n",
    "        net.base -= 10000*g_base\n",
    "        \n",
    "    loss = nan_mse_loss(net(), target)  \n",
    "    g_coeffs, = grad(loss, net.coeffs)\n",
    "    with torch.no_grad():\n",
    "        net.coeffs -= 10000*g_coeffs\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(loss.item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008309215772897005\n",
      "0.0008298605098389089\n",
      "0.0008291483391076326\n",
      "0.0008285042131319642\n",
      "0.000827873358502984\n",
      "0.0008272448903881013\n",
      "0.0008266165386885405\n",
      "0.0008259877213276923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-8e3cae354934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnan_mse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mg_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         inputs, allow_unused)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    output = net()\n",
    "    loss = nan_mse_loss(output, target)\n",
    "    g_base, = grad(loss, net.base)\n",
    "    with torch.no_grad():\n",
    "        net.base -= 100*g_base\n",
    "        \n",
    "    output = net()\n",
    "    loss = nan_mse_loss(output, target)  \n",
    "    g_coeffs, = grad(loss, net.coeffs)\n",
    "    with torch.no_grad():\n",
    "        net.coeffs -= 100*g_coeffs\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3293317556381226\n",
      "100 0.0003069194790441543\n",
      "200 0.00012027848424622789\n",
      "300 8.65106558194384e-05\n",
      "400 7.552202441729605e-05\n",
      "500 6.944664346519858e-05\n",
      "600 6.26373803243041e-05\n",
      "700 5.832554597873241e-05\n",
      "800 5.610979496850632e-05\n",
      "900 5.295785376802087e-05\n",
      "1000 5.0605838623596355e-05\n",
      "1100 4.8676676669856533e-05\n",
      "1200 4.73498112114612e-05\n",
      "1300 4.697476833825931e-05\n",
      "1400 4.684360101236962e-05\n",
      "1500 4.676807293435559e-05\n",
      "1600 4.6717213990632445e-05\n",
      "1700 4.66806668555364e-05\n",
      "1800 4.665327651309781e-05\n",
      "1900 4.66321362182498e-05\n",
      "2000 4.6615452447440475e-05\n",
      "2100 4.6602017391705886e-05\n",
      "2200 4.65909943159204e-05\n",
      "2300 4.658179750549607e-05\n",
      "2400 4.657400131691247e-05\n",
      "2500 4.6567296521971e-05\n",
      "2600 4.656145756598562e-05\n",
      "2700 4.65563171019312e-05\n",
      "2800 4.655174780054949e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-40165be2324e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class AE2(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE2, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*400*400, bias=False)\n",
    "        self.coeffs = nn.Linear(1, self.n_comps*self.n_coeffs, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        coeffs = self.coeffs(x).view(self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ki,kj->ji', base.view(self.n_comps,400*400), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "net = AE2(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.1)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "for i in range(3000):\n",
    "    output = net(input)\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
