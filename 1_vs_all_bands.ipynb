{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ds = xr.open_dataset(\"/data/pca_act/000_clean.nc\")\n",
    "\n",
    "stack = np.empty((0,400,400))\n",
    "ti_nan = (np.count_nonzero(np.isnan(ds.nbart_blue.values), axis=(1,2)))<.66*160000\n",
    "ds = ds.isel(time=ti_nan)\n",
    "#np.save(f\"m3_times\", ds.time.values)\n",
    "\n",
    "#for fname in ds:\n",
    "for fname in ['nbart_red']:\n",
    "    band = ds[fname].values/1e4\n",
    "    stack = np.append(stack, band, axis=0)\n",
    "    \n",
    "stack = stack.reshape(stack.shape[0], -1)\n",
    "\n",
    "print(np.nanmax(stack))\n",
    "print(stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "    \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*404*404, bias=False)\n",
    "        self.conv1 = nn.Conv2d(self.n_comps, 2*self.n_comps, kernel_size=5, bias=False)\n",
    "        self.coeffs = nn.Linear(1, 2*self.n_comps*self.n_coeffs, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        conv1 =  torch.tanh(self.conv1(base.reshape(1, self.n_comps, 404, 404)))   \n",
    "        coeffs = self.coeffs(x).view(2*self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ik,kj->ji', conv1.view(400*400, 2*self.n_comps), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ncomps = 12\n",
    "\n",
    "ncoeffs = stack.shape[0]\n",
    "\n",
    "input = torch.ones(1, device=device)\n",
    "tmean = np.nanmean(stack, axis=0)\n",
    "#np.save(f\"m3_mean\", tmean)\n",
    "\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "net = AE(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10000\n",
    "    \n",
    "for it in range(epochs):\n",
    "    # training loop:\n",
    "    output = net(input)\n",
    "\n",
    "    #sparsity = 0.00001 * torch.linalg.norm(net.coeffs.weight, 1)\n",
    "    #magnitude = torch.linalg.norm(torch.linalg.norm(list(net.parameters())[0], 'fro') - 1, 2)\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print(it, loss.item(), nan_mse_loss(output, target).item())\n",
    "        \n",
    "        \n",
    "params = list(net.parameters())\n",
    "for param in params:\n",
    "    print(param.shape)\n",
    "\n",
    "base = params[0].cpu().detach().numpy()\n",
    "coeffs = params[1].cpu().detach().numpy()\n",
    "\n",
    "#np.save(f\"m3_base\", base)\n",
    "#np.save(f\"m3_coeffs\", coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "    \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, n_comps, n_coeffs):\n",
    "        super(AE, self).__init__()\n",
    "        self.n_coeffs = n_coeffs\n",
    "        self.n_comps = n_comps\n",
    "        self.D = nn.Linear(1, self.n_comps*404*404)\n",
    "        self.conv1 = nn.Conv2d(self.n_comps, 2*self.n_comps, kernel_size=5, bias=False)\n",
    "        self.coeffs = nn.Linear(1, 2*self.n_comps*self.n_coeffs, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        base = self.D(x)\n",
    "        conv1 =  torch.tanh(self.conv1(base.reshape(1, self.n_comps, 404, 404)))   \n",
    "        coeffs = self.coeffs(x).view(2*self.n_comps, self.n_coeffs)\n",
    "        return torch.einsum('ik,kj->ji', conv1.view(400*400, 2*self.n_comps), coeffs)\n",
    "\n",
    "\n",
    "def nan_mse_loss(output, target):\n",
    "    loss = torch.mean((output[target == target] - target[target == target])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "ncomps = 12\n",
    "\n",
    "ncoeffs = stack.shape[0]\n",
    "\n",
    "input = torch.ones(1, device=device)\n",
    "tmean = np.nanmean(stack, axis=0)\n",
    "#np.save(f\"m3_mean\", tmean)\n",
    "\n",
    "target = torch.from_numpy(stack-tmean).float().to(device)\n",
    "\n",
    "net = AE(ncomps, ncoeffs)\n",
    "net.to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10000\n",
    "    \n",
    "for it in range(epochs):\n",
    "    # training loop:\n",
    "    output = net(input)\n",
    "\n",
    "    #sparsity = 0.00001 * torch.linalg.norm(net.coeffs.weight, 1)\n",
    "    #magnitude = torch.linalg.norm(torch.linalg.norm(list(net.parameters())[0], 'fro') - 1, 2)\n",
    "    loss = nan_mse_loss(output, target)# + sparsity\n",
    "\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss.backward()\n",
    "    optimizer.step()    # does the update\n",
    "\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print(it, loss.item(), nan_mse_loss(output, target).item())\n",
    "        \n",
    "        \n",
    "params = list(net.parameters())\n",
    "for param in params:\n",
    "    print(param.shape)\n",
    "\n",
    "base = params[0].cpu().detach().numpy()\n",
    "coeffs = params[1].cpu().detach().numpy()\n",
    "\n",
    "#np.save(f\"m3_base\", base)\n",
    "#np.save(f\"m3_coeffs\", coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "stack_hat = net(input).cpu().detach().numpy().reshape(-1,400,400)\n",
    "plt.imshow(stack_hat[30])\n",
    "\n",
    "stack_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(np.square(stack[:71,:].reshape(-1,400,400)-(stack_hat[:71,:]+tmean.reshape(400,400))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "stack_hat = net(input).cpu().detach().numpy().reshape(-1,400,400)\n",
    "plt.imshow(stack_hat[60])\n",
    "\n",
    "stack_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(np.square(stack[:71,:].reshape(-1,400,400)-(stack_hat[:71,:]+tmean.reshape(400,400))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(stack_hat[30]+tmean.reshape(400,400),vmin=0,vmax=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((stack).reshape(-1,400,400)[30],vmin=0,vmax=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"00_00_net.py\")\n",
    "net.eval()\n",
    "\n",
    "stack_hat = net(input).cpu().detach().numpy().reshape(-1,400,400) + np.load(\"00_00_mean.npy\").reshape(400,400)\n",
    "print(stack_hat.shape)\n",
    "plt.imshow(8*np.dstack((stack_hat[60],stack_hat[71+60],stack_hat[2*71+60])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
